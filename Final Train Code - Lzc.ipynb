{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.model_selection import KFold\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = joblib.load(\"train_data.pkl\")\n",
    "targets, data, ids = loaded_data\n",
    "\n",
    "# 打印某个人的具体信息\n",
    "for i in tqdm(range(277)):\n",
    "    if ids[i] == \"03d92c9f6f8a\":\n",
    "        print(f\"Target: {targets[i]}\")\n",
    "        print(f\"{data[i]}\")\n",
    "        print(type(targets[i]))\n",
    "        print(type(data[i]))\n",
    "\n",
    "for i in tqdm(range(277)):\n",
    "    print(ids[i], end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_events = pd.read_csv(\"./train_events.csv\")\n",
    "num_person = len(train_events[\"series_id\"].unique())\n",
    "print(\"The number of total persons is:\", num_person)\n",
    "\n",
    "person_ids = train_events[\"series_id\"].unique()\n",
    "ids_to_index = {person_id: index for index, person_id in enumerate(person_ids)}\n",
    "index_to_ids = {index: person_id for index, person_id in enumerate(person_ids)}\n",
    "print(ids_to_index)\n",
    "print(index_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    window_overlap = 36\n",
    "    window_size = 18\n",
    "    batch_size = 32\n",
    "    input_dim = 2\n",
    "    embed_dim = 32\n",
    "    num_classes = 4 \n",
    "    num_layers = 4\n",
    "    nhead = 4\n",
    "    dim_feedforward = 64\n",
    "    learning_rate = 1e-3\n",
    "    epochs = 300\n",
    "    train_record_steps = 1\n",
    "    dropout = 0.2\n",
    "    num_person = 277\n",
    "    scheduler_step_size = 30\n",
    "    scheduler_gamma = 0.8\n",
    "    fold = 8\n",
    "    weight_decay = 0.001\n",
    "    model_dir = \"./models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将每个人的数据分别提取出来\n",
    "\n",
    "dic_X = {}\n",
    "dic_y = {}\n",
    "\n",
    "for i in tqdm(range(min(num_person, CFG.num_person))):\n",
    "\n",
    "    print(\"Id:\", ids[i])\n",
    "    person_id = ids[i]\n",
    "\n",
    "    dic_X[person_id] = []\n",
    "    dic_y[person_id] = []\n",
    "\n",
    "    pair_list = targets[i]\n",
    "    onset_steps = []\n",
    "    wakeup_steps = []\n",
    "    for idx in range(len(pair_list)):\n",
    "        onset_steps.append(pair_list[idx][0])\n",
    "        wakeup_steps.append(pair_list[idx][1])\n",
    "\n",
    "    empty_spaces = []\n",
    "    events = train_events[train_events[\"series_id\"] == person_id]\n",
    "    last_events = 0\n",
    "    empty_mark = 0 \n",
    "    for (idx, row) in events.iterrows():\n",
    "        if np.isnan(row[\"step\"]):\n",
    "            empty_mark = 1\n",
    "        else:\n",
    "            if row[\"step\"] > last_events and empty_mark == 0:\n",
    "                last_events = row[\"step\"]\n",
    "            elif row[\"step\"] > last_events and empty_mark == 1:\n",
    "                empty_spaces.append((int(last_events) - 360, int(row[\"step\"]) - 360))\n",
    "                last_events = row[\"step\"]\n",
    "                empty_mark = 0\n",
    "\n",
    "    df = data[i]\n",
    "    labels = []\n",
    "    for j in range(0, len(df), CFG.window_overlap):\n",
    "        start = j\n",
    "        if j + CFG.window_size >= len(df):\n",
    "            start = len(df) - CFG.window_size\n",
    "            end = len(df)\n",
    "            break\n",
    "        else:\n",
    "            end = j + CFG.window_size\n",
    "\n",
    "        flag_empty = 0\n",
    "        for k in empty_spaces:\n",
    "            if (k[0] < start < k[1]) or (k[0] < end < k[1]):\n",
    "                flag_empty = 1\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        flag_sleep = 0\n",
    "        for k in range(len(onset_steps)):\n",
    "            if (onset_steps[k] < start < end < wakeup_steps[k]):\n",
    "                flag_sleep = 1\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "\n",
    "        chunk = df.iloc[start:end][[\"anglez\", \"enmo\"]]\n",
    "        chunk = chunk.to_numpy()\n",
    "\n",
    "        if flag_empty == 1:\n",
    "            label = 4\n",
    "        elif any(start <= num <= end for num in wakeup_steps):\n",
    "            label = 3\n",
    "        elif any(start <= num <= end for num in onset_steps):\n",
    "            label = 2\n",
    "        elif flag_sleep == 1:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        labels.append(label)\n",
    "\n",
    "        if label != 4:\n",
    "            dic_X[person_id].append(chunk)\n",
    "            dic_y[person_id].append(label)\n",
    "\n",
    "    element_counts = Counter(labels)\n",
    "\n",
    "    # for element, count in element_counts.items():\n",
    "    #     print(f\"Element {element} occurs {count} times\")\n",
    "\n",
    "    print(len(dic_X[person_id]), len(dic_y[person_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in dic_y.items():\n",
    "    y = value\n",
    "    X = dic_X[key]\n",
    "\n",
    "    X_awake = []\n",
    "    y_awake = []\n",
    "    X_sleep = []\n",
    "    y_sleep = []\n",
    "    X_onset = []\n",
    "    y_onset = []\n",
    "    X_wakeup = []\n",
    "    y_wakeup = []\n",
    "\n",
    "    for idx in range(len(y)):\n",
    "        if y[idx] == 0:\n",
    "            X_awake.append(X[idx])\n",
    "            y_awake.append(y[idx])\n",
    "        elif y[idx] == 1:\n",
    "            X_sleep.append(X[idx])\n",
    "            y_sleep.append(y[idx])\n",
    "        elif y[idx] == 2:\n",
    "            X_onset.append(X[idx])\n",
    "            y_onset.append(y[idx])\n",
    "        elif y[idx] == 3:\n",
    "            X_wakeup.append(X[idx])\n",
    "            y_wakeup.append(y[idx])\n",
    "\n",
    "    num_sample = max(len(X_onset), len(X_wakeup))\n",
    "\n",
    "    indices = random.sample(range(len(X_awake)), num_sample)\n",
    "    selected_X_awake = [X_awake[i] for i in indices]\n",
    "    selected_y_awake = [y_awake[i] for i in indices]\n",
    "    X_awake = selected_X_awake\n",
    "    y_awake = selected_y_awake\n",
    "\n",
    "    indices = random.sample(range(len(X_sleep)), num_sample)\n",
    "    selected_X_sleep = [X_sleep[i] for i in indices]\n",
    "    selected_y_sleep = [y_sleep[i] for i in indices]\n",
    "    X_sleep = selected_X_sleep\n",
    "    y_sleep = selected_y_sleep\n",
    "\n",
    "    # print(len(X_awake), len(X_sleep), len(X_onset), len(X_wakeup))\n",
    "\n",
    "    X_total = []\n",
    "    y_total = []\n",
    "    X_total = X_awake + X_sleep + X_onset + X_wakeup\n",
    "    y_total = y_awake + y_sleep + y_onset + y_wakeup\n",
    "\n",
    "    dic_y[key] = y_total\n",
    "    dic_X[key] = X_total\n",
    "\n",
    "    print(len(dic_X[key]), len(dic_y[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, inputs, outputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        inp = self.inputs[index]\n",
    "        output = self.outputs[index]\n",
    "\n",
    "        input_tensor = torch.tensor(inp, dtype=torch.float32)\n",
    "        output_tensor = torch.tensor(output, dtype=torch.long)\n",
    "\n",
    "        return input_tensor, output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        embed_dim,\n",
    "        num_classes=4,\n",
    "        num_layers=8,\n",
    "        nhead=4,\n",
    "        dim_feedforward=64,\n",
    "        dropout=0.2\n",
    "    ):\n",
    "\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=input_dim, \n",
    "            out_channels=embed_dim, \n",
    "            kernel_size=3, \n",
    "            padding=1\n",
    "        )\n",
    "        self.embed_layer = nn.Linear(embed_dim, embed_dim)\n",
    "        self.layernorm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=embed_dim,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                dropout=dropout\n",
    "            ),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.classification = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1d(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = self.embed_layer(x)\n",
    "        x = self.layernorm(x)\n",
    "        x = self.encoder(x.permute(1, 0, 2))\n",
    "        x = x[-1]\n",
    "        x = self.classification(x)\n",
    "        return x\n",
    "\n",
    "# 示例\n",
    "model = TransformerModel(input_dim=2, embed_dim=32)\n",
    "input_data = torch.rand(3, 360, 2)\n",
    "output = model(input_data)\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerModel(\n",
    "    input_dim=CFG.input_dim,\n",
    "    embed_dim=CFG.embed_dim,\n",
    "    num_classes=CFG.num_classes,\n",
    "    num_layers=CFG.num_layers,\n",
    "    nhead=CFG.nhead,\n",
    "    dim_feedforward=CFG.dim_feedforward,\n",
    "    dropout=CFG.dropout\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_series = pd.read_parquet(\"./train_series.parquet\")\n",
    "\n",
    "def inference(person_id, model):\n",
    "    df = train_series[train_series[\"series_id\"] == person_id]\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    for j in range(0, len(df), CFG.window_overlap):\n",
    "        start = j\n",
    "        if j + CFG.window_size >= len(df):\n",
    "            start = len(df) - CFG.window_size\n",
    "            end = len(df)\n",
    "            break\n",
    "        else:\n",
    "            end = j + CFG.window_size\n",
    "            \n",
    "        chunk = df.iloc[start:end][[\"anglez\", \"enmo\"]]\n",
    "        chunk = chunk.to_numpy()\n",
    "        X.append(chunk)\n",
    "        y.append(-1)\n",
    "\n",
    "    dataset = MyDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=CFG.batch_size, shuffle=False)\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "    for batch_idx, (data, target) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            print(predicted)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_step_losses = []\n",
    "test_losses = []\n",
    "learning_rates = []\n",
    "\n",
    "indices = list(range(CFG.num_person))\n",
    "kf = KFold(n_splits=CFG.fold, shuffle=True, random_state=2023)\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(indices)):\n",
    "    # print(f\"Fold {fold + 1}\")\n",
    "    print(\"Train indices:\", train_index)\n",
    "    print(\"Test indices:\", val_index)\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for idx in train_index:\n",
    "        person_id = index_to_ids[idx]\n",
    "        X_train.extend(dic_X[person_id])\n",
    "        y_train.extend(dic_y[person_id])\n",
    "\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    for idx in val_index:\n",
    "        person_id = index_to_ids[idx]\n",
    "        X_val.extend(dic_X[person_id])\n",
    "        y_val.extend(dic_y[person_id])\n",
    "    \n",
    "    print(len(X_train), len(X_val))\n",
    "\n",
    "    train_dataset = MyDataset(X_train, y_train)\n",
    "    val_dataset = MyDataset(X_val, y_val)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=CFG.batch_size, shuffle=False)\n",
    "\n",
    "    train_model = model.to(device=device)\n",
    "    train_model.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(train_model.parameters(), lr=CFG.learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=CFG.scheduler_step_size, gamma=CFG.scheduler_gamma)\n",
    "\n",
    "    min_loss = np.inf\n",
    "\n",
    "    for epoch in range(1, CFG.epochs + 1):\n",
    "        train_loss = 0.0\n",
    "        test_loss = 0.0\n",
    "        train_step_loss = 0.0\n",
    "\n",
    "        learning_rate = optimizer.param_groups[0][\"lr\"]\n",
    "        learning_rates.append(learning_rate)\n",
    "\n",
    "        for batch_idx, (data, target) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = train_model(data)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            train_loss += loss.item()\n",
    "            train_step_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # if (batch_idx + 1) % CFG.train_record_steps == 0:\n",
    "            #     train_step_losses.append(train_step_loss / (batch_idx + 1))\n",
    "            #     print(f\"Fold: {fold + 1}, Epoch: {epoch}, Step: {(batch_idx + 1)}, Train Loss Now: {train_step_loss / (batch_idx + 1)}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in tqdm(enumerate(val_dataloader), total=len(val_dataloader)):\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "                output = train_model(data)\n",
    "\n",
    "                loss = criterion(output, target)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "            for idx in val_index:\n",
    "                person_id = index_to_ids[idx]\n",
    "                predictions = inference(person_id=person_id, model=train_model)\n",
    "                print(predictions)\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        test_loss /= len(val_dataloader)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(f\"Fold: {fold + 1}, Epoch: {epoch}, Train Loss: {train_loss}, Test Loss: {test_loss}\")\n",
    "\n",
    "        if test_loss < min_loss:\n",
    "            min_loss = test_loss\n",
    "            torch.save(train_model.state_dict(), f\"{CFG.model_dir}/model_epoch_{fold + 1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train & Test Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(learning_rates)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "\n",
    "plt.savefig(\"plot.png\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
